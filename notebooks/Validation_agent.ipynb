{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33780061-8151-4396-bf46-b4cafd37b32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d1b0f03d514508854efcce895f598c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e3c3b5729e45e687593a12c21ed7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "from assurhabitat_agents.model.llm_model_loading import llm_inference\n",
    "from assurhabitat_agents.config.tool_config import VALIDATION_TOOLS, VALIDATION_TOOLS_DESCRIPTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb9ca80-919f-4beb-b85d-856af2d060a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationReActState(TypedDict):\n",
    "    images_path: list[str]\n",
    "    history: list[str]  # L'historique des échanges (Thought, Action, Observation)\n",
    "    last_action: str | None  # Le nom de l'outil à appeler (si applicable)\n",
    "    last_arguments: dict | None  # Les arguments à passer à l'outil\n",
    "    last_observation: str | None  # Le résultat de l'outil appelé\n",
    "    \n",
    "    parsed_declaration: dict  # Resultat de l'agent Declaration (Ne peut pas etre None, Le superviseur ne peut appeler cet agent que si cet etat est connu)\n",
    "\n",
    "    # results from tools\n",
    "    image_conformity: dict | None       # {\"match\": bool, \"raw_output\": str}\n",
    "    guarantee_report: dict | None       # {\"is_garanteed\": bool, \"guarantee\": {...}}\n",
    "    answer: str | None  # Finale answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5f973b-415c-47cf-976c-d3128d8e5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_valid(state: ValidationReActState, tools) -> str:\n",
    "    \n",
    "    HISTORY_KEEP = 10\n",
    "    history = state.get(\"history\", [])[-HISTORY_KEEP:]\n",
    "\n",
    "    # Show parsed_declaration and missing fields if available\n",
    "    parsed = state.get(\"parsed_declaration\")\n",
    "    conformity = state.get(\"image_conformity\")\n",
    "    guarantee = state.get(\"guarantee_report\")\n",
    "    images = state.get(\"images_path\", [])\n",
    "\n",
    "    # Build actions block\n",
    "    actions_block = \"\\n\".join(f\"- {a}\" for a in tools) if tools else \"- (no tools available)\"\n",
    "\n",
    "    parts = [\n",
    "        \"You are the Validation Agent for AssurHabitat. Decide the next step: either\",\n",
    "        \"1) call a tool (Action) OR 2) give the final answer (Réponse).\",\n",
    "        \"\",\n",
    "        \"Available tools:\",\n",
    "        actions_block,\n",
    "        \"Tool descriptions:\",\n",
    "        VALIDATION_TOOLS_DESCRIPTION,\n",
    "        \"\",\n",
    "        \"Rules:\",\n",
    "        \"- If you call a tool, use a single line: Action: TOOL_NAME\",\n",
    "        \"- If arguments are needed, write: Arguments: then either a JSON object or key=value lines\",\n",
    "        \"- If you return the final reply to the user, write: Réponse: <text>\",\n",
    "        \"\",\n",
    "        \"Decision rules:\",\n",
    "        \"- If image_conformity is None: you MUST call CheckConformity first.\",\n",
    "        \"- If guarantee_report is None but image_conformity.match == True: then call CheckGuarantee.\",\n",
    "        \"- If both are completed: produce a final 'Réponse:' for the supervisor.\",\n",
    "        \"\",\n",
    "        \"Context summary:\",\n",
    "    ]\n",
    "\n",
    "    if history:\n",
    "        parts.append(\"Recent history:\")\n",
    "        parts.append(\"\\n\".join(history))\n",
    "    if parsed:\n",
    "        # pretty print the parsed_declaration small snippet\n",
    "        try:\n",
    "            pretty = json.dumps(parsed, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            pretty = str(parsed)\n",
    "        parts.append(\"Current parsed_declaration JSON: (you can find the sinistre type inside for CheckConformity)\")\n",
    "        parts.append(pretty)\n",
    "    if conformity:\n",
    "        parts.append(\"Conformity: \" + json.dumps(conformity, ensure_ascii=False))\n",
    "        \n",
    "    parts.append(\"Images available in the state (use them for CheckConformity):\")\n",
    "    parts.append(json.dumps(images, ensure_ascii=False))\n",
    "    \n",
    "    if guarantee:\n",
    "        parts.append(\"Guarantee: \" + json.dumps(guarantee, ensure_ascii=False))\n",
    "\n",
    "    parts.append(\"\")\n",
    "    parts.append(\"Now propose the next single Thought + Action (or final Réponse).\")\n",
    "    # join and return\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "804ea1df-668d-4cc5-a964-e65d70cd6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(output: str):\n",
    "    text = output.strip()\n",
    "\n",
    "    # Action\n",
    "    m_action = re.search(r\"(?mi)^Action:\\s*(?P<tool>[^\\n]+)\", text)\n",
    "    m_args = re.search(r\"(?mi)^Arguments:\\s*(?P<args>[\\s\\S]+)$\", text)\n",
    "\n",
    "    if m_action:\n",
    "        tool = m_action.group(\"tool\").strip()\n",
    "        args = {}\n",
    "\n",
    "        if m_args:\n",
    "            raw = m_args.group(\"args\").strip()\n",
    "\n",
    "            # Trim noise\n",
    "            for token in [\"Observation\", \"LLM output\", \"Thought\", \"Action\"]:\n",
    "                idx = raw.find(token)\n",
    "                if idx > 0:\n",
    "                    raw = raw[:idx].strip()\n",
    "\n",
    "            # JSON only (lists OR dicts)\n",
    "            try:\n",
    "                parsed = json.loads(raw)\n",
    "                if not isinstance(parsed, dict):\n",
    "                    raise ValueError(\"Arguments must be a JSON object.\")\n",
    "                args = parsed\n",
    "            except Exception:\n",
    "                raise ValueError(f\"Invalid JSON arguments: {raw}\")\n",
    "\n",
    "        return (\"action\", tool, args)\n",
    "\n",
    "    # Final response\n",
    "    m_resp = re.search(r\"(?mi)^(Réponse|Answer):\\s*(?P<ans>[\\s\\S]+)$\", text)\n",
    "    if m_resp:\n",
    "        return (\"answer\", m_resp.group(\"ans\").strip(), None)\n",
    "\n",
    "    return (\"thought\", text, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cbfc85-0916-4e01-a920-4f2b08cc4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = VALIDATION_TOOLS\n",
    "tool_names = list(VALIDATION_TOOLS.keys())\n",
    "\n",
    "def node_thought_action(state: ValidationReActState) -> ValidationReActState:\n",
    "\n",
    "    prompt = format_prompt_valid(state, tool_names)\n",
    "    output = llm_inference(prompt)\n",
    "\n",
    "    # parse_output must return a tuple like (\"action\", tool_name, tool_args)\n",
    "    # or (\"answer\", answer_text) or (\"thought\", thought_text)\n",
    "    step_type, *content = parse_output(output)\n",
    "\n",
    "    # Append the raw LLM output to history for traceability\n",
    "    state.setdefault(\"history\", [])\n",
    "    state[\"history\"].append(f\"LLM output: {output}\")\n",
    "\n",
    "    if step_type == \"action\":\n",
    "        tool_name, tool_args = content\n",
    "        # store next action and its arguments\n",
    "        state[\"last_action\"] = tool_name\n",
    "        state[\"last_arguments\"] = tool_args or {}\n",
    "        # keep history friendly: record the action intention\n",
    "        state[\"history\"].append(f\"Action: call tool: {tool_name} with args: {tool_args}\")\n",
    "    elif step_type == \"answer\":\n",
    "        # final textual answer produced by the LLM\n",
    "        state[\"last_action\"] = None\n",
    "        state[\"last_arguments\"] = None\n",
    "        state[\"last_observation\"] = None\n",
    "        state[\"history\"].append(f\"Answer: {content[0]}\")\n",
    "        state[\"answer\"] = content[0]\n",
    "    else:\n",
    "        # Thought only: no action requested, we keep loop running\n",
    "        state[\"history\"].append(f\"Thought: {content[0] if content else ''}\")\n",
    "    return state\n",
    "\n",
    "def node_tool_execution(state: ValidationReActState) -> ValidationReActState:\n",
    "    \"\"\"\n",
    "    Execute the tool stored in state['last_action'] with state['last_arguments'].\n",
    "    Update state['last_observation'], state['history'], and structured fields:\n",
    "      - state['image_conformity']\n",
    "      - state['guarantee_report']\n",
    "    \"\"\"\n",
    "    tool_name = state.get(\"last_action\")\n",
    "    tool_args = state.get(\"last_arguments\") or {}\n",
    "\n",
    "    # nothing to execute\n",
    "    if not tool_name:\n",
    "        state.setdefault(\"history\", []).append(\"No action to execute.\")\n",
    "        return state\n",
    "\n",
    "    # call the tool if available\n",
    "    if tool_name in tools:\n",
    "        try:\n",
    "            observation = tools[tool_name](**tool_args)\n",
    "        except Exception as e:\n",
    "            observation = f\"Error during tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        observation = f\"Error: Unknown tool {tool_name}\"\n",
    "\n",
    "    # store observation and history\n",
    "    state[\"last_observation\"] = str(observation)\n",
    "    state.setdefault(\"history\", []).append(f\"Observation from {tool_name}: {state['last_observation']}\")\n",
    "\n",
    "    if tool_name == \"CheckConformity\":\n",
    "        if isinstance(observation, dict):\n",
    "            state['image_conformity'] = observation\n",
    "        else:\n",
    "            state[\"history\"].append(\"check_conformity failed.\")\n",
    "            state['image_conformity'] = {\n",
    "                    \"error\": \"CheckConformity failed.\",\n",
    "                    \"reason\": str(observation)\n",
    "                }\n",
    "\n",
    "    # ---------- CASE 2: verify_completeness tool (explicit call) ----------\n",
    "    elif tool_name == \"CheckGuarantee\":\n",
    "        if isinstance(observation, dict):\n",
    "            state['guarantee_report'] = observation\n",
    "        else:\n",
    "            # Only overwrite if guarantee_report was empty\n",
    "            if not state.get('guarantee_report'):\n",
    "                state['guarantee_report'] = {\n",
    "                    \"error\": \"CheckGuarantee failed.\",\n",
    "                    \"reason\": str(observation)\n",
    "                }\n",
    "            state[\"history\"].append(\"CheckGuarantee failed.\")\n",
    "\n",
    "    # reset action so next Thought node computes next step\n",
    "    state[\"last_action\"] = None\n",
    "    state[\"last_arguments\"] = None\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d2634a-9139-4d0a-b0ff-afc576a6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    graph_builder = StateGraph(ValidationReActState)\n",
    "    graph_builder.add_node(\"thought\", node_thought_action)\n",
    "    graph_builder.add_node(\"action\", node_tool_execution)\n",
    "\n",
    "    graph_builder.add_edge(START, \"thought\")\n",
    "\n",
    "    def decide_from_thought(runtime_state: ValidationReActState):\n",
    "            if runtime_state.get(\"answer\"):\n",
    "                return END\n",
    "            if runtime_state.get(\"last_action\"):\n",
    "                return \"action\"\n",
    "            return \"thought\"\n",
    "\n",
    "    graph_builder.add_conditional_edges(\"thought\", decide_from_thought)\n",
    "    graph_builder.add_edge(\"action\", \"thought\")\n",
    "    return graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5daf9f9a-4833-48e9-b193-985114cabccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_graph(graph, initial_state: ValidationReActState, max_steps: int = 10):\n",
    "    \"\"\"\n",
    "    Generic runner for the compiled graph.\n",
    "    - graph: result of build_graph(...). It must provide a `run_once(state)` or we emulate node execution.\n",
    "    If your StateGraph API differs, adapt accordingly.\n",
    "    \"\"\"\n",
    "    state = initial_state\n",
    "    step = 0\n",
    "\n",
    "    # Pretty print function\n",
    "    def print_new_history(prev_len):\n",
    "        history = state.get(\"history\", [])\n",
    "        for line in history[prev_len:]:\n",
    "            print(line)\n",
    "        return len(history)\n",
    "\n",
    "    prev_history_len = 0\n",
    "    while step < max_steps:\n",
    "        step += 1\n",
    "        state = node_thought_action(state)\n",
    "        prev_history_len = print_new_history(prev_history_len)\n",
    "\n",
    "        if state.get(\"guarantee_report\") and state.get(\"image_conformity\") or state.get(\"answer\"):\n",
    "            break\n",
    "\n",
    "        if state.get(\"last_action\"):\n",
    "            state = node_tool_execution(state)\n",
    "            prev_history_len = print_new_history(prev_history_len)\n",
    "            # continue loop, next iteration Thought will run again\n",
    "        else:\n",
    "            # if no action and not complete, allow loop to continue (LLM might set action next)\n",
    "            # small sleep to avoid busy loop in notebook (optional)\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "    # final\n",
    "    print(\"\\n--- FINAL STATE ---\")\n",
    "    print(\"guarantee_report:\", state.get(\"guarantee_report\"))\n",
    "    print(\"image_conformity:\", state.get(\"image_conformity\"))\n",
    "    print(\"final answer:\", state.get(\"answer\"))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cd4aff-82bf-4b39-8933-8523827b7fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01bd03b94794aea8cafd437ddb364d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (SpecialTokenPolicy.IGNORE) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: Thought: The image_conformity is None, so I need to call CheckConformity first.\n",
      "\n",
      "Action: CheckConformity\n",
      "Arguments: {\"image_paths\": [\"/workspace/Assurhabitat_agents/data/FireDamage_7.png\"], \"parsed_declaration\": {\"sinistre_type\": \"vol_vandalisme\", \"sinistre_confidence\": 0.99, \"sinistre_explain\": \"cambriolage via vélux, appareils électroniques volés\", \"candidates\": [{\"type\": \"vol_vandalisme\", \"score\": 0.99}], \"extracted\": {\"date_sinistre\": \"2024-06-13\", \"lieu\": \"chambre\", \"description\": \"cambriolage via vélux, appareils électroniques volés\", \"photos\": [\"/workspace/Assurhabitat_agents/data/FireDamage_7.png\"], \"biens_impactes\": [\"appareils électroniques\"], \"police_report_number\": \"123456789\"}}}\n",
      "Action: call tool: CheckConformity with args: {'image_paths': ['/workspace/Assurhabitat_agents/data/FireDamage_7.png'], 'parsed_declaration': {'sinistre_type': 'vol_vandalisme', 'sinistre_confidence': 0.99, 'sinistre_explain': 'cambriolage via vélux, appareils électroniques volés', 'candidates': [{'type': 'vol_vandalisme', 'score': 0.99}], 'extracted': {'date_sinistre': '2024-06-13', 'lieu': 'chambre', 'description': 'cambriolage via vélux, appareils électroniques volés', 'photos': ['/workspace/Assurhabitat_agents/data/FireDamage_7.png'], 'biens_impactes': ['appareils électroniques'], 'police_report_number': '123456789'}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d8c9cb32c4ad98ece51b77aa9cc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation from CheckConformity: {'match': False, 'description': 'The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.', 'raw_output': '{\\n  \"description\": \"The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.\",\\n  \"match\": false\\n}'}\n",
      "LLM output: Thought: The image_conformity.match is False, so there is no need to call CheckGuarantee. I will provide the final response to the supervisor.\n",
      "\n",
      "Réponse: The image does not match the declared sinistre type of \"vol_vandalisme\". The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.\n",
      "Answer: The image does not match the declared sinistre type of \"vol_vandalisme\". The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.\n",
      "\n",
      "--- FINAL STATE ---\n",
      "guarantee_report: None\n",
      "image_conformity: {'match': False, 'description': 'The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.', 'raw_output': '{\\n  \"description\": \"The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.\",\\n  \"match\": false\\n}'}\n",
      "final answer: The image does not match the declared sinistre type of \"vol_vandalisme\". The image shows a room with a damaged fireplace and carpet, with black soot and debris scattered on the floor. There is also a damaged vacuum cleaner and some debris on the floor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (SpecialTokenPolicy.IGNORE) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"images_path\": [\"/workspace/Assurhabitat_agents/data/FireDamage_7.png\"],\n",
    "    \"history\": [],\n",
    "    \"last_action\": None,\n",
    "    \"last_arguments\": None,\n",
    "    \"last_observation\": None,\n",
    "    \"parsed_declaration\": {'sinistre_type': 'vol_vandalisme', \n",
    "                           'sinistre_confidence': 0.99, \n",
    "                           'sinistre_explain': 'cambriolage via vélux, appareils électroniques volés', \n",
    "                           'candidates': [{'type': 'vol_vandalisme', 'score': 0.99}], \n",
    "                           'extracted': {'date_sinistre': '2024-06-13', \n",
    "                                         'lieu': 'chambre', \n",
    "                                         'description': 'cambriolage via vélux, appareils électroniques volés', \n",
    "                                         'photos': [\"/workspace/Assurhabitat_agents/data/FireDamage_7.png\"], \n",
    "                                         'biens_impactes': ['appareils électroniques'], \n",
    "                                         'police_report_number': '123456789'}},\n",
    "    \"image_conformity\": None,\n",
    "    \"guarantee_report\": None\n",
    "}\n",
    "\n",
    "graph = build_graph()\n",
    "\n",
    "final_state = run_graph(graph, initial_state, max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a01e-7a24-4c1a-b8b7-4eabcd41d911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
