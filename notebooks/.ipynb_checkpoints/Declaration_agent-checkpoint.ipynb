{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d4579f",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c28663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement curl (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for curl\u001b[0m\u001b[31m\n",
      "\u001b[0m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5255  100  5255    0     0  24446      0 --:--:-- --:--:-- --:--:-- 24441\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2341  100  2341    0     0  10747      0 --:--:-- --:--:-- --:--:-- 10788\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4113k  100 4113k    0     0  8946k      0 --:--:-- --:--:-- --:--:-- 8942k\n",
      "unzip:  cannot find or open ../data, ../data.zip or ../data.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!pip install curl unzip\n",
    "!mkdir -p ../data\n",
    "!curl -L -o ../data/docA.md https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/00ba97/Agentic+Chatbot+Assurance+Habitation+-+Processus.md\n",
    "!curl -L -o ../data/docB.md https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/00ba97/Agentic+Chatbot+Assurance+Habitation+-+Garanties.md\n",
    "!curl -L -o ../data/attachments.zip https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/00ba97/attachments.zip\n",
    "!unzip ../data ../data/attachments.zip\n",
    "!rm ../data/attachments.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df9a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4d1b9fe8254565913923ea9d58b1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eacb2c12d84b7fbca1bf62e8516973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tekken.json:   0%|          | 0.00/19.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ac203382a4dbba5ec6169aea24c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb614acb8484c22a68e2df95291d60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58255cf1231a4d679b6296b6164ce0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0637d1898cb84635b99642e62dc08bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da4abc421d04e12b192854adcbaf2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503b9f90a32040919fb6e85f9203ce0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc846cb04f042af9c2b0aae277c584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794e950f81534a6eb85b398236148bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6198d5df8843c48ef161b4ae6ec227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00010.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d4cecb3c6145c0ab6df599264d0bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00010.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d35076a6974e7a99b0170cf3307d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3815cf49ac7b4475ac803c217221c5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00010.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8033d676f947428a9fddd26945e042a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00010.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d6ac8807054f1e8df1754ff227a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c0abdf2b784e1782abf35f31e1d90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "from assurhabitat_agents.model.llm_model_loading import llm_inference\n",
    "from assurhabitat_agents.config.tool_config import DECLARATION_TOOLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843294df",
   "metadata": {},
   "source": [
    "# Declaration agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6753fa",
   "metadata": {},
   "source": [
    "## Tools\n",
    "2. parse_declaration\n",
    "3. verify_completeness\n",
    "4. ask_human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d691b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeclarationReActState(TypedDict):\n",
    "    question: str  # La question initiale de l'utilisateur\n",
    "    history: list[str]  # L'historique des échanges (Thought, Action, Observation)\n",
    "    last_action: str | None  # Le nom de l'outil à appeler (si applicable)\n",
    "    last_arguments: dict | None  # Les arguments à passer à l'outil\n",
    "    last_observation: str | None  # Le résultat de l'outil appelé\n",
    "    is_complete: bool | None  # La réponse finale\n",
    "    parsed_declaration: dict | None # Stockage json de la declaration parser par le tool\n",
    "    missing: list[str] | None # champs manquant dans la declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e847ea0",
   "metadata": {},
   "source": [
    "## Nodes\n",
    "1. thought\n",
    "2. execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e24bf7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(state: DeclarationReActState, tools) -> str:\n",
    "    \"\"\"\n",
    "    Build a concise prompt for the ReAct LLM using the whole state.\n",
    "    - state: the DeclarationReActState dict (contains history, parsed_declaration, missing, question, etc.)\n",
    "    - actions: list of available tool names with short descriptions (e.g. [\"parse_declaration\", \"verify_completeness\", \"ask_human\"])\n",
    "    The function returns a prompt string ready to be sent to the LLM.\n",
    "    \"\"\"\n",
    "    # Keep prompt short: only include last few history entries\n",
    "    HISTORY_KEEP = 10\n",
    "    history = state.get(\"history\", [])[-HISTORY_KEEP:]\n",
    "\n",
    "    # Show parsed_declaration and missing fields if available\n",
    "    parsed = state.get(\"parsed_declaration\")\n",
    "    missing = state.get(\"missing\", [])\n",
    "\n",
    "    # Build actions block\n",
    "    actions_block = \"\\n\".join(f\"- {a}\" for a in tools) if tools else \"- (no tools available)\"\n",
    "\n",
    "    parts = [\n",
    "        \"You are the Declaration Agent for AssurHabitat. Decide the next step: either\",\n",
    "        \"1) call a tool (Action) OR 2) give the final answer (Réponse).\",\n",
    "        \"\",\n",
    "        \"Available tools:\",\n",
    "        actions_block,\n",
    "        \"\",\n",
    "        \"Rules:\",\n",
    "        \"- If you call a tool, use a single line: Action: TOOL_NAME\",\n",
    "        \"- If arguments are needed, write: Arguments: then either a JSON object or key=value lines\",\n",
    "        \"- If you return the final reply to the user, write: Réponse: <text>\",\n",
    "        \"\",\n",
    "        \"Context summary:\",\n",
    "    ]\n",
    "\n",
    "    if state.get(\"question\"):\n",
    "        parts.append(f\"Original question: {state['question']}\")\n",
    "    if history:\n",
    "        parts.append(\"Recent history:\")\n",
    "        parts.append(\"\\n\".join(history))\n",
    "    if parsed:\n",
    "        # pretty print the parsed_declaration small snippet\n",
    "        try:\n",
    "            pretty = json.dumps(parsed, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            pretty = str(parsed)\n",
    "        parts.append(\"Current parsed_declaration JSON:\")\n",
    "        parts.append(pretty)\n",
    "    if missing:\n",
    "        parts.append(\"Missing fields (need to ask human if required):\")\n",
    "        parts.append(\", \".join(missing))\n",
    "\n",
    "    parts.append(\"\")\n",
    "    parts.append(\"Now propose the next single Thought + Action (or final Réponse).\")\n",
    "    # join and return\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3969463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(output: str) -> Tuple[str, Any, Any]:\n",
    "    \"\"\"\n",
    "    Parse LLM output and return a tuple:\n",
    "    - (\"action\", tool_name, tool_args_dict)\n",
    "    - (\"answer\", answer_text, None)\n",
    "    - (\"thought\", thought_text, None)\n",
    "\n",
    "    This parser is tolerant:\n",
    "    - accepts \"Action: TOOLNAME\" on a line\n",
    "    - accepts arguments as either JSON after \"Arguments:\" OR key=value lines\n",
    "    - if parsing fails for args, returns them as raw string under {\"raw\": \"...\"}\n",
    "    \"\"\"\n",
    "    text = output.strip()\n",
    "\n",
    "    # Try to find an \"Action:\" line (match up to end-of-line, non-greedy)\n",
    "    m_action = re.search(r\"(?mi)^Action:\\s*(?P<tool>[^\\n\\r]+)\", text)\n",
    "    m_args = re.search(r\"(?mi)^Arguments:\\s*(?P<args>[\\s\\S]+)$\", text)  # capture until string end\n",
    "\n",
    "    # If action present, parse args if any\n",
    "    if m_action:\n",
    "        tool_name = m_action.group(\"tool\").strip()\n",
    "        tool_args = {}\n",
    "\n",
    "        if m_args:\n",
    "            raw_args = m_args.group(\"args\").strip()\n",
    "            # Try JSON first\n",
    "            # after finding raw_args:\n",
    "            # cut off if raw_args contains \"Observation\" or \"LLM output\" (heuristic)\n",
    "            cut_tokens = [\"Observation from\", \"LLM output\", \"Action:\", \"Thought:\"]\n",
    "            for t in cut_tokens:\n",
    "                idx = raw_args.find(t)\n",
    "                if idx != -1:\n",
    "                    raw_args = raw_args[:idx].strip()\n",
    "                    break\n",
    "            try:\n",
    "                parsed = json.loads(raw_args)\n",
    "                if isinstance(parsed, dict):\n",
    "                    tool_args = parsed\n",
    "                else:\n",
    "                    tool_args = {\"raw\": parsed}\n",
    "            except Exception:\n",
    "                # Fallback: parse key=value lines\n",
    "                lines = [l.strip() for l in raw_args.splitlines() if l.strip()]\n",
    "                kv = {}\n",
    "                for line in lines:\n",
    "                    # accept \"key = value\" or \"key=value\"\n",
    "                    m_kv = re.match(r\"^\\s*([^=]+?)\\s*=\\s*(.+)$\", line)\n",
    "                    if m_kv:\n",
    "                        key = m_kv.group(1).strip()\n",
    "                        val = m_kv.group(2).strip()\n",
    "                        # try to interpret JSON value (numbers, lists, etc.)\n",
    "                        try:\n",
    "                            val_parsed = json.loads(val)\n",
    "                        except Exception:\n",
    "                            val_parsed = val\n",
    "                        kv[key] = val_parsed\n",
    "                    else:\n",
    "                        # can't parse line -> keep raw under a list\n",
    "                        kv.setdefault(\"_raw_lines\", []).append(line)\n",
    "                tool_args = kv if kv else {\"raw\": raw_args}\n",
    "\n",
    "        return (\"action\", tool_name, tool_args)\n",
    "\n",
    "    # If there's a \"Réponse:\" or \"Answer:\" line, treat as final answer\n",
    "    m_answer = re.search(r\"(?mi)^(Réponse|Answer):\\s*(?P<ans>[\\s\\S]+)$\", text)\n",
    "    if m_answer:\n",
    "        return (\"answer\", m_answer.group(\"ans\").strip(), None)\n",
    "\n",
    "    # Try to parse JSON directly as answer/action\n",
    "    try:\n",
    "        j = json.loads(text)\n",
    "        if isinstance(j, dict):\n",
    "            # if dict contains action key, map to action\n",
    "            if \"action\" in j:\n",
    "                return (\"action\", j.get(\"action\"), j.get(\"args\", {}))\n",
    "            if \"answer\" in j:\n",
    "                return (\"answer\", j.get(\"answer\"), None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # otherwise fallback to thought\n",
    "    return (\"thought\", text, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2a181b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = DECLARATION_TOOLS\n",
    "tool_names = list(DECLARATION_TOOLS.keys())\n",
    "\n",
    "def node_thought_action(state: DeclarationReActState) -> DeclarationReActState:\n",
    "    \"\"\"\n",
    "    Node that produces the next Thought/Action/Answer using the LLM.\n",
    "    It fills last_action/last_arguments when the LLM asks to call a tool,\n",
    "    or writes the final answer when the LLM produces an 'answer'.\n",
    "    \"\"\"\n",
    "    # Build the prompt using the state's history and some structured context\n",
    "    # It's helpful to include parsed_declaration and missing fields in the prompt so the LLM\n",
    "    # can reason clearly about the next step.\n",
    "    prompt = format_prompt(state, tool_names)\n",
    "    output = llm_inference(prompt)\n",
    "\n",
    "    # parse_output must return a tuple like (\"action\", tool_name, tool_args)\n",
    "    # or (\"answer\", answer_text) or (\"thought\", thought_text)\n",
    "    step_type, *content = parse_output(output)\n",
    "\n",
    "    # Append the raw LLM output to history for traceability\n",
    "    state.setdefault(\"history\", [])\n",
    "    state[\"history\"].append(f\"LLM output: {output}\")\n",
    "\n",
    "    if step_type == \"action\":\n",
    "        tool_name, tool_args = content\n",
    "        # store next action and its arguments\n",
    "        state[\"last_action\"] = tool_name\n",
    "        state[\"last_arguments\"] = tool_args or {}\n",
    "        # keep history friendly: record the action intention\n",
    "        state[\"history\"].append(f\"Action: call tool: {tool_name} with args: {tool_args}\")\n",
    "    elif step_type == \"answer\":\n",
    "        # final textual answer produced by the LLM\n",
    "        state[\"is_complete\"] = True\n",
    "        state[\"last_action\"] = None\n",
    "        state[\"last_arguments\"] = None\n",
    "        state[\"last_observation\"] = None\n",
    "        state[\"history\"].append(f\"Answer: {content[0]}\")\n",
    "    else:\n",
    "        # Thought only: no action requested, we keep loop running\n",
    "        state[\"history\"].append(f\"Thought: {content[0] if content else ''}\")\n",
    "    return state\n",
    "\n",
    "def node_tool_execution(state: DeclarationReActState) -> DeclarationReActState:\n",
    "    \"\"\"\n",
    "    Execute the tool stored in state['last_action'] with state['last_arguments'].\n",
    "    Update state['last_observation'], state['history'], and structured fields:\n",
    "      - state['parsed_declaration']\n",
    "      - state['is_complete'], state['missing'] via verify_completeness(parsed_declaration)\n",
    "    Behavior for ask_human:\n",
    "      - If parse_declaration tool exists, we call it with a combined raw input that\n",
    "        contains the old parsed JSON and the new human reply so the LLM can merge them.\n",
    "      - Otherwise, a simple heuristic fills the first missing field with the reply.\n",
    "    \"\"\"\n",
    "    tool_name = state.get(\"last_action\")\n",
    "    tool_args = state.get(\"last_arguments\") or {}\n",
    "\n",
    "    # nothing to execute\n",
    "    if not tool_name:\n",
    "        state.setdefault(\"history\", []).append(\"No action to execute.\")\n",
    "        return state\n",
    "\n",
    "    # call the tool if available\n",
    "    if tool_name in TOOLS:\n",
    "        try:\n",
    "            observation = TOOLS[tool_name](**tool_args)\n",
    "        except Exception as e:\n",
    "            observation = f\"Error during tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        observation = f\"Error: Unknown tool {tool_name}\"\n",
    "\n",
    "    # store observation and history\n",
    "    state[\"last_observation\"] = str(observation)\n",
    "    state.setdefault(\"history\", []).append(f\"Observation from {tool_name}: {state['last_observation']}\")\n",
    "\n",
    "    if tool_name == \"DeclarationParser\":\n",
    "        if isinstance(observation, dict):\n",
    "            # Replace entire parsed_declaration with returned dict\n",
    "            state[\"parsed_declaration\"] = observation\n",
    "\n",
    "            # After parsing, run verify_completeness if available\n",
    "            if \"InformationVerification\" in TOOLS:\n",
    "                try:\n",
    "                    verify_res = TOOLS[\"InformationVerification\"](state[\"parsed_declaration\"])\n",
    "                    if isinstance(verify_res, dict):\n",
    "                        state[\"is_complete\"] = bool(verify_res.get(\"is_complete\", False))\n",
    "                        state[\"missing\"] = verify_res.get(\"missing\", [])\n",
    "                        state[\"history\"].append(f\"Auto-verify result: {verify_res}\")\n",
    "                except Exception as e:\n",
    "                    state[\"history\"].append(f\"Auto-verify failed: {e}\")\n",
    "        else:\n",
    "            state[\"history\"].append(\"parse_declaration returned non-dict observation.\")\n",
    "\n",
    "    # ---------- CASE 2: verify_completeness tool (explicit call) ----------\n",
    "    elif tool_name == \"InformationVerification\":\n",
    "        if isinstance(observation, dict):\n",
    "            state[\"is_complete\"] = bool(observation.get(\"is_complete\", False))\n",
    "            state[\"missing\"] = observation.get(\"missing\", [])\n",
    "        else:\n",
    "            state[\"history\"].append(\"verify_completeness returned unexpected output.\")\n",
    "\n",
    "    # ---------- CASE 3: ask_human tool (human response) ----------\n",
    "    elif tool_name == \"AskHuman\":\n",
    "        # observation expected to be the human reply string (or similar)\n",
    "        human_reply = observation if isinstance(observation, str) else str(observation)\n",
    "        state[\"history\"].append(f\"Human replied: {human_reply}\")\n",
    "\n",
    "        # If parse_declaration tool exists, call it with merged input:\n",
    "        # Build combined raw input: include previous parsed_declaration JSON and the new human reply.\n",
    "        if \"DeclarationParser\" in TOOLS and isinstance(state.get(\"parsed_declaration\"), dict):\n",
    "            # Convert previous parsed_declaration to compact JSON and instruct the LLM to merge\n",
    "            prev_json = json.dumps(state[\"parsed_declaration\"], ensure_ascii=False)\n",
    "            combined_raw_input = (\n",
    "                \"Existing parsed JSON:\\n\" + prev_json + \"\\n\\n\"\n",
    "                \"New user input (please update the JSON using this new information):\\n\"\n",
    "                + human_reply\n",
    "                + \"\\n\\n\"\n",
    "                \"- If the input contains already a JSON and new information, add the new information to the old JSON and return the new JSON.\"\n",
    "            )\n",
    "            try:\n",
    "                merged_obs = TOOLS[\"DeclarationParser\"](combined_raw_input)\n",
    "                # If the parse_declaration returns dict, update parsed_declaration and re-run verify\n",
    "                if isinstance(merged_obs, dict):\n",
    "                    state[\"parsed_declaration\"] = merged_obs\n",
    "\n",
    "                    # call verify_completeness automatically\n",
    "                    if \"InformationVerification\" in TOOLS:\n",
    "                        try:\n",
    "                            verify_res = TOOLS[\"InformationVerification\"](state[\"parsed_declaration\"])\n",
    "                            if isinstance(verify_res, dict):\n",
    "                                state[\"is_complete\"] = bool(verify_res.get(\"is_complete\", False))\n",
    "                                state[\"missing\"] = verify_res.get(\"missing\", [])\n",
    "                                state[\"history\"].append(f\"Auto-verify after human reply: {verify_res}\")\n",
    "                        except Exception as e:\n",
    "                            state[\"history\"].append(f\"Auto-verify failed after human reply: {e}\")\n",
    "\n",
    "                    # Clear asked missing fields or remove those filled by LLM\n",
    "                    # We keep the current 'missing' returned by verify_completeness.\n",
    "                else:\n",
    "                    # If parse_declaration did not return dict, fallback: simple fill\n",
    "                    if state.get(\"missing\"):\n",
    "                        first = state[\"missing\"][0]\n",
    "                        state.setdefault(\"parsed_declaration\", {}).setdefault(\"extracted\", {})[first] = human_reply\n",
    "                        state[\"missing\"] = state.get(\"missing\", [])[1:]\n",
    "                        state[\"history\"].append(f\"Filled {first} with human reply (fallback).\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # on error, fallback to naive update\n",
    "                if state.get(\"missing\"):\n",
    "                    first = state[\"missing\"][0]\n",
    "                    state.setdefault(\"parsed_declaration\", {}).setdefault(\"extracted\", {})[first] = human_reply\n",
    "                    state[\"missing\"] = state.get(\"missing\", [])[1:]\n",
    "                    state[\"history\"].append(f\"Filled {first} with human reply (fallback due to error: {e}).\")\n",
    "        else:\n",
    "            # No parse tool available -> naive fill into first missing field\n",
    "            if state.get(\"missing\"):\n",
    "                first = state[\"missing\"][0]\n",
    "                state.setdefault(\"parsed_declaration\", {}).setdefault(\"extracted\", {})[first] = human_reply\n",
    "                state[\"missing\"] = state.get(\"missing\", [])[1:]\n",
    "                state[\"history\"].append(f\"Filled {first} with human reply (no parse tool).\")\n",
    "\n",
    "    # reset action so next Thought node computes next step\n",
    "    state[\"last_action\"] = None\n",
    "    state[\"last_arguments\"] = None\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372dfee",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e0a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    graph_builder = StateGraph(DeclarationReActState)\n",
    "    graph_builder.add_node(\"thought\", node_thought_action)\n",
    "    graph_builder.add_node(\"action\", node_tool_execution)\n",
    "\n",
    "    graph_builder.add_edge(START, \"thought\")\n",
    "\n",
    "    def decide_from_thought(runtime_state: DeclarationReActState):\n",
    "            if runtime_state.get(\"is_complete\"):\n",
    "                return END\n",
    "            if runtime_state.get(\"last_action\"):\n",
    "                return \"action\"\n",
    "            return END\n",
    "\n",
    "    graph_builder.add_conditional_edges(\"thought\", decide_from_thought)\n",
    "    graph_builder.add_edge(\"action\", \"thought\")\n",
    "    return graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e87db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_graph(graph, initial_state: DeclarationReActState, max_steps: int = 30):\n",
    "    \"\"\"\n",
    "    Generic runner for the compiled graph.\n",
    "    - graph: result of build_graph(...). It must provide a `run_once(state)` or we emulate node execution.\n",
    "    If your StateGraph API differs, adapt accordingly.\n",
    "    \"\"\"\n",
    "    state = initial_state\n",
    "    step = 0\n",
    "\n",
    "    # Pretty print function\n",
    "    def print_new_history(prev_len):\n",
    "        history = state.get(\"history\", [])\n",
    "        for line in history[prev_len:]:\n",
    "            print(line)\n",
    "        return len(history)\n",
    "\n",
    "    prev_history_len = 0\n",
    "    while step < max_steps:\n",
    "        step += 1\n",
    "        state = node_thought_action(state)\n",
    "        prev_history_len = print_new_history(prev_history_len)\n",
    "\n",
    "        if state.get(\"is_complete\") or state.get(\"answer\"):\n",
    "            break\n",
    "\n",
    "        if state.get(\"last_action\"):\n",
    "            state = node_tool_execution(state)\n",
    "            prev_history_len = print_new_history(prev_history_len)\n",
    "            # continue loop, next iteration Thought will run again\n",
    "        else:\n",
    "            # if no action and not complete, allow loop to continue (LLM might set action next)\n",
    "            # small sleep to avoid busy loop in notebook (optional)\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "    # final\n",
    "    print(\"\\n--- FINAL STATE ---\")\n",
    "    print(\"is_complete:\", state.get(\"is_complete\"))\n",
    "    print(\"parsed_declaration:\", state.get(\"parsed_declaration\"))\n",
    "    print(\"missing:\", state.get(\"missing\"))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47e0bb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: Thought: The user has reported a burglary through the skylight of their bedroom, with electronic devices stolen. I need to verify the details of the incident to proceed with the declaration.\n",
      "\n",
      "Action: InformationVerification\n",
      "Action: call tool: InformationVerification with args: {}\n",
      "Observation from InformationVerification: Error during tool InformationVerification: verify_completeness() missing 1 required positional argument: 'parsed_declaration'\n",
      "verify_completeness returned unexpected output.\n",
      "LLM output: Thought: The error indicates that the InformationVerification tool requires a parsed declaration as an argument. I need to first parse the user's declaration using the DeclarationParser tool.\n",
      "\n",
      "Action: DeclarationParser\n",
      "Action: call tool: DeclarationParser with args: {}\n",
      "Observation from DeclarationParser: Error during tool DeclarationParser: parse_declaration() missing 1 required positional argument: 'raw_input'\n",
      "parse_declaration returned non-dict observation.\n",
      "LLM output: Thought: I need to provide the raw input from the user's message to the DeclarationParser tool to parse the declaration correctly.\n",
      "\n",
      "Action: DeclarationParser\n",
      "Arguments: raw_input=\"Bonjour, on m'a cambriolé ce matin, les voleurs sont passés par le vélux de la chambre et ont volé tous les appareils électroniques. Merci de me contacter rapidement.\"\n",
      "Action: call tool: DeclarationParser with args: {'raw_input': \"Bonjour, on m'a cambriolé ce matin, les voleurs sont passés par le vélux de la chambre et ont volé tous les appareils électroniques. Merci de me contacter rapidement.\"}\n",
      "Observation from DeclarationParser: {'sinistre_type': 'vol_vandalisme', 'sinistre_confidence': 0.99, 'sinistre_explain': 'cambriolage via vélux, appareils électroniques volés', 'candidates': [{'type': 'vol_vandalisme', 'score': 0.99}], 'extracted': {'date_sinistre': None, 'lieu': 'chambre', 'description': 'cambriolage via vélux, appareils électroniques volés', 'photos': [], 'biens_impactes': ['appareils électroniques'], 'police_report_number': None}}\n",
      "Auto-verify result: {'is_complete': False, 'missing': ['date_sinistre', 'police_report_number']}\n",
      "LLM output: Thought: The parsed declaration is incomplete as it lacks the 'date_sinistre' and 'police_report_number'. I need to ask the user for this information to complete the declaration.\n",
      "\n",
      "Action: AskHuman\n",
      "Arguments: message=\"Pour compléter votre déclaration, pourriez-vous s'il vous plaît fournir la date du sinistre et le numéro de rapport de police si vous en avez un ?\"\n",
      "Action: call tool: AskHuman with args: {'message': \"Pour compléter votre déclaration, pourriez-vous s'il vous plaît fournir la date du sinistre et le numéro de rapport de police si vous en avez un ?\"}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pour compléter votre déclaration, pourriez-vous s'il vous plaît fournir la date du sinistre et le numéro de rapport de police si vous en avez un ? 13 juin 2024 et le mumero est 123456789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation from AskHuman: 13 juin 2024 et le mumero est 123456789\n",
      "Human replied: 13 juin 2024 et le mumero est 123456789\n",
      "Auto-verify after human reply: {'is_complete': True, 'missing': []}\n",
      "LLM output: Thought: The declaration is now complete with all necessary information. I will provide the final answer to the user.\n",
      "\n",
      "Réponse: Votre déclaration a été enregistrée avec succès. Voici les détails de votre sinistre :\n",
      "- Type de sinistre : Vol/Vandalisme\n",
      "- Date du sinistre : 13 juin 2024\n",
      "- Lieu : Chambre\n",
      "- Description : Cambriolage via vélux, appareils électroniques volés\n",
      "- Numéro de rapport de police : 123456789\n",
      "- Biens impactés : Appareils électroniques\n",
      "\n",
      "Merci de votre coopération. Nous vous contacterons prochainement pour la suite des démarches.\n",
      "Answer: Votre déclaration a été enregistrée avec succès. Voici les détails de votre sinistre :\n",
      "- Type de sinistre : Vol/Vandalisme\n",
      "- Date du sinistre : 13 juin 2024\n",
      "- Lieu : Chambre\n",
      "- Description : Cambriolage via vélux, appareils électroniques volés\n",
      "- Numéro de rapport de police : 123456789\n",
      "- Biens impactés : Appareils électroniques\n",
      "\n",
      "Merci de votre coopération. Nous vous contacterons prochainement pour la suite des démarches.\n",
      "\n",
      "--- FINAL STATE ---\n",
      "is_complete: True\n",
      "parsed_declaration: {'sinistre_type': 'vol_vandalisme', 'sinistre_confidence': 0.99, 'sinistre_explain': 'cambriolage via vélux, appareils électroniques volés', 'candidates': [{'type': 'vol_vandalisme', 'score': 0.99}], 'extracted': {'date_sinistre': '2024-06-13', 'lieu': 'chambre', 'description': 'cambriolage via vélux, appareils électroniques volés', 'photos': [], 'biens_impactes': ['appareils électroniques'], 'police_report_number': '123456789'}}\n",
      "missing: []\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"question\": \"Bonjour, on m'a cambriolé ce matin, les voleurs sont passés par le vélux de la \"\n",
    "    \"chambre et ont volé tous les appareils électroniques. Merci de me contacter rapidement.\",\n",
    "    \"history\": [],\n",
    "    \"last_action\": None,\n",
    "    \"last_arguments\": None,\n",
    "    \"last_observation\": None,\n",
    "    \"is_complete\": False,\n",
    "    \"parsed_declaration\": None,\n",
    "    \"missing\": []\n",
    "}\n",
    "\n",
    "graph = build_graph()\n",
    "\n",
    "final_state = run_graph(graph, initial_state, max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8415e6-ee4a-4598-ac67-013d34f6905f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
