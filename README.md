# ğŸ  AssurHabitat Agents

**AssurHabitat Agents** is an AI-driven multi-agent system designed to automate and support **home insurance claim processing**.  
The project combines **LLMs, Computer Vision, multi-agent orchestration, LangGraph, and Langfuse** to cover the full lifecycle of an insurance claim â€” from initial declaration to validation, expertise, cost estimation, and evaluation.

---

## Key Features

- **Declaration Agent**  
  Extracts structured information from user claim declarations (text + images).
- **Validation Agent**  
  Verifies image conformity and factual consistency with the declared claim.
- **Expertise Agent**  
  Performs expert reasoning, business logic validation, and tool-based analysis.
- **Orchestration Agent**  
  Manages the end-to-end workflow and selects the most appropriate service provider.
- **Observability & Tracing**  
  Full tracing of tool calls, decisions, and errors using **Langfuse**.
- **Agent Evaluation Framework**  
  Golden Datasetâ€“based evaluation with automated metrics.

---

## System Architecture
![diagram](doc/diagram.png)

---

## Project Goals

- Automate complex insurance claim workflows  
- Ensure AI decisions are explainable and traceable  
- Enable fast iteration on prompts, tools, and agents  
- Follow modern LLMOps / AgentOps best practices  

---

## Technical Stack & Design Choices

- **LangGraph** â€” multi-agent orchestration and state machines  
- **Langfuse** â€” observability, datasets, and evaluation  
- **HuggingFace** â€” LLMs and Vision-Language Models  
- **Python 3.11**  
- **Tool-based, agentic architecture**

---

### Install dependencies

```bash
pip install -r requirements.txt
```

Create .env file with variables HF_TOKEN, LANGFUSE_SECRET_KEY, LANGFUSE_PUBLIC_KEY, LANGFUSE_BASE_URL then
```bash
make setup
```

Run the projet
```bash
make run
```

---

## Observability with Langfuse

The project relies on Langfuse for:
- Tracing every LLM and VLM call
- Tracking tool usage and agent decisions
- Debugging failures and unexpected behaviors
- Running dataset-based evaluations

Centralized configuration is located in: src/assurhabitat_agents/config/langfuse_config.py

The @observe() decorator is used across agents and tools to enable automatic tracing.

---
## Agent Evaluation

### Golden Dataset

The `eval/golden_dataset.json` file contains **9 reference scenarios** (3 per claim family), each including:

- User input (text + images)
- Expected outputs for each agent
- Business-grounded reference decisions

Claim families covered:
- ğŸ”¥ Fire / Explosion
- ğŸ’§ Water Damage
- ğŸ”“ Theft / Vandalism

---

### ğŸ“Š Evaluation Metrics

| Agent | Metric |
|------|-------|
| Declaration Agent | Information completeness |
| Validation Agent | Imageâ€“claim consistency |
| Expertise Agent | Manual expert review |

---

### Run the Evaluation

Run the full evaluation pipeline on the golden dataset:

```bash
cd src/
python ../eval/run_golden_evaluation.py
```

---

## Testing

The project includes comprehensive unit tests covering all agents, tools, and edge cases.

### Running Tests

```bash
# Run all tests
make test

# Run with coverage report
make test-coverage

# Run specific test suites
make test-agents    # Test only agents
make test-tools     # Test only tools

# Run tests in verbose mode
make test-verbose
```

Or using pytest directly:

```bash
# All tests
pytest tests/

# Specific test file
pytest tests/agents/test_declaration_agent.py -v

# With coverage
pytest tests/ --cov=src/assurhabitat_agents --cov-report=html
```

### Test Structure

```
tests/
â”œâ”€â”€ agents/           # Agent tests (52+ tests)
â”œâ”€â”€ tools/            # Tool tests (39+ tests)
â”œâ”€â”€ fixtures/         # Test fixtures and mocks
â”œâ”€â”€ conftest.py       # Pytest configuration
â””â”€â”€ README.md         # Testing documentation
```

**Test Coverage:**
- âœ… Declaration Agent (complete/incomplete cases, edge cases)
- âœ… Validation Agent (conformity checks, guarantee validation)
- âœ… Expertise Agent (cost estimation, report generation)
- âœ… Orchestrator (end-to-end flows, error handling)
- âœ… All tools (parsing, conformity, guarantee checks)

For detailed testing documentation, see [`tests/README.md`](tests/README.md).

---

## Roadmap (Ideas)

- Improved automatic scoring metrics  
- Domain-specific fine-tuning, particularly for expertise and cost estimation
- PDF claim report generation  

---

## Author

Developed by **Remi Nollet** â€” AI, Computer Vision, LLM & Agentic Systems engineer

---

## License

Educational and demonstrator project  
Free to use for learning, experimentation, and demos.
